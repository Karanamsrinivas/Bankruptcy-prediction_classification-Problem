{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n#Import the libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import f1_score, make_scorer, accuracy_score, recall_score, precision_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils import class_weight\nfrom sklearn.feature_selection import RFE\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import plot_confusion_matrix\nfrom keras.models import Sequential\nfrom tensorflow.keras.models import Sequential\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.neural_network import MLPClassifier\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\n\n\nimport random\nimport itertools\n\nimport scipy\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-05T20:28:27.177352Z","iopub.execute_input":"2023-04-05T20:28:27.177929Z","iopub.status.idle":"2023-04-05T20:28:43.255387Z","shell.execute_reply.started":"2023-04-05T20:28:27.177886Z","shell.execute_reply":"2023-04-05T20:28:43.253515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1- Data overview","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/company-bankruptcy-prediction/data.csv')","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:28:43.258941Z","iopub.execute_input":"2023-04-05T20:28:43.259839Z","iopub.status.idle":"2023-04-05T20:28:43.661388Z","shell.execute_reply.started":"2023-04-05T20:28:43.259793Z","shell.execute_reply":"2023-04-05T20:28:43.659223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:28:43.663675Z","iopub.execute_input":"2023-04-05T20:28:43.664135Z","iopub.status.idle":"2023-04-05T20:28:43.723715Z","shell.execute_reply.started":"2023-04-05T20:28:43.664095Z","shell.execute_reply":"2023-04-05T20:28:43.721785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Shape of Data:\",data.shape)\nr, c = data.shape\nprint(\"Number of Rows:\",r)\nprint(\"Number of Columns:\",c)","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:28:43.727187Z","iopub.execute_input":"2023-04-05T20:28:43.727713Z","iopub.status.idle":"2023-04-05T20:28:43.738288Z","shell.execute_reply.started":"2023-04-05T20:28:43.72766Z","shell.execute_reply":"2023-04-05T20:28:43.736037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Information about the Dataset\")\ndata.info()","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:28:43.740321Z","iopub.execute_input":"2023-04-05T20:28:43.74085Z","iopub.status.idle":"2023-04-05T20:28:43.796451Z","shell.execute_reply.started":"2023-04-05T20:28:43.740798Z","shell.execute_reply":"2023-04-05T20:28:43.794272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of Null Values:\",data.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:28:43.801998Z","iopub.execute_input":"2023-04-05T20:28:43.802599Z","iopub.status.idle":"2023-04-05T20:28:43.817227Z","shell.execute_reply.started":"2023-04-05T20:28:43.80254Z","shell.execute_reply":"2023-04-05T20:28:43.815136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of Duplicate Values: \",data.duplicated().sum())","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:28:43.819864Z","iopub.execute_input":"2023-04-05T20:28:43.820302Z","iopub.status.idle":"2023-04-05T20:28:43.887658Z","shell.execute_reply.started":"2023-04-05T20:28:43.820255Z","shell.execute_reply":"2023-04-05T20:28:43.886295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Descriptive Stastices\")\ndata.describe()","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:28:43.889607Z","iopub.execute_input":"2023-04-05T20:28:43.890485Z","iopub.status.idle":"2023-04-05T20:28:44.209305Z","shell.execute_reply.started":"2023-04-05T20:28:43.890445Z","shell.execute_reply":"2023-04-05T20:28:44.207175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2- Target variable analysis","metadata":{}},{"cell_type":"markdown","source":"The target is a dichotomous variable, I am going to have a look at the distribution of the two classes.","metadata":{}},{"cell_type":"code","source":"print(data[\"Bankrupt?\"].value_counts())\nplt.figure()\nsns.countplot(x = 'Bankrupt?',data = data )\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:28:44.211304Z","iopub.execute_input":"2023-04-05T20:28:44.211699Z","iopub.status.idle":"2023-04-05T20:28:44.473502Z","shell.execute_reply.started":"2023-04-05T20:28:44.211664Z","shell.execute_reply":"2023-04-05T20:28:44.471254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Note: There is a huge imbalance between the two categories. It turns out that only 3.2% companies in this dataset bankrupted.**","metadata":{}},{"cell_type":"markdown","source":"# 3-  Variable-target analysis","metadata":{}},{"cell_type":"markdown","source":"Some companies bankrupted and some did not. However, before proceeding with the analysis I would like to see at least a small evidence that the variables have effect on the bankrupcy.","metadata":{}},{"cell_type":"markdown","source":"> **3.1 Non-statistical test**\n\nPlotting the relative difference between the means of the features for both categories (bankrupted and not bankrupted).","metadata":{}},{"cell_type":"code","source":"#Variables' effect on class\n\nfeatures = data.columns[1:] #from now on \"features\" are interchangable with \"columns\"\n\nX = data[features]\ny = data[\"Bankrupt?\"]\n\nX_0 = X.loc[y==0,:] #not bankrupted\nX_1 = X.loc[y==1,:] #bakrupted\n\nX_0_test = X_0.sample(n=220)\n\nsignificant_cols = [] #features that have \"very different\" means\ndifs=[] #differences between means\n\nfor col in X.columns:\n    relative_means_difference = (X_1[col].mean() - X_0_test[col].mean()) / X_0_test[col].mean() \n    difs.append([col,relative_means_difference])\n    if abs(relative_means_difference)>0.5: #tresnhold, at least 50% freater/smaller mean \n        significant_cols.append(col)\n\n\nsns.barplot(x=list(range(len(difs))),y=[e[1] for e in difs])\nplt.ylim((-1,5)) #this controls the size of the window displayed\nplt.xlabel(\"Features\")\nplt.ylabel(\"Relative difference between means\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:28:44.478335Z","iopub.execute_input":"2023-04-05T20:28:44.478826Z","iopub.status.idle":"2023-04-05T20:28:45.95836Z","shell.execute_reply.started":"2023-04-05T20:28:44.478789Z","shell.execute_reply":"2023-04-05T20:28:45.956691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are a few features with really big differences and overall around 20 features whose means are more than 50% apart in these two categories.","metadata":{}},{"cell_type":"markdown","source":"> **3.2 Monte Carlo Hypothesis Test**","metadata":{}},{"cell_type":"markdown","source":"**HYPOTHESIS**: There is a difference between bakrupted and not-bankrupted companiesÂ¶\n(Null hypothesis: There is no difference between the bankrupted and not-bankrupted companies.)\n\nI am going to generate 1000 samples, each containing 220 datapoints from X (- all datapoints) and obtain the sampling distribution of the sample mean for each feature. From the observed data (= the 220 datapoints of bankrupted companies) and sampling distribution I am going to determine the p-value.\n\n**p-value** for each feature: percentage of sample means that are more extreme than the bankrupt companies mean","metadata":{}},{"cell_type":"code","source":"#MONTE CARLO HYPOTESIS TEST\n\nfrom statistics import mean\n\nsampling_distribution = {feature: [] for feature in features} #SAMPLING DISTRIBUTION OF SAMPLE MEANS for each feature\nbankrupt_means = {feature: X_1[feature].mean() for feature in features} #MEAN of each feature (observed data = bankrupt companies)\n\nfor i in range(1000): #sampling from the data 1000 times\n    X_sample = X.sample(n=220) #n same as the number of bankrupt companies,sampling from X\n    for feature in features:\n        s_mean = X_sample[feature].mean()\n        sampling_distribution[feature].append(s_mean)\n\npvalues = {feature: None for feature in features}\n\ndef get_p_value(sampling_distribution, observed):\n    l = abs(observed-mean(sampling_distribution)) #distance of observed from the sample mean\n    return sum(abs(sample_mean-mean(sampling_distribution))>l for sample_mean in sampling_distribution)/len(sampling_distribution) #the proportion of data more extreme than observed\n               \nfor feature in pvalues: #filling the pvalues dictionary\n    pvalues[feature] = get_p_value(sampling_distribution[feature],bankrupt_means[feature]) ","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:28:45.962454Z","iopub.execute_input":"2023-04-05T20:28:45.962879Z","iopub.status.idle":"2023-04-05T20:32:00.880142Z","shell.execute_reply.started":"2023-04-05T20:28:45.962842Z","shell.execute_reply":"2023-04-05T20:32:00.878851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of significantly different features: %d\" %sum(np.array(list(pvalues.values()))>0.05))\ndict(itertools.islice(pvalues.items(),10)) #look at the first 10 features and associated p-values","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:32:00.881991Z","iopub.execute_input":"2023-04-05T20:32:00.882853Z","iopub.status.idle":"2023-04-05T20:32:00.893924Z","shell.execute_reply.started":"2023-04-05T20:32:00.882801Z","shell.execute_reply":"2023-04-05T20:32:00.892611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plotting some features and their distribution of sample means + red line with the mean of the observed data (= data of bankrupt companies)\n\nfig, axes = plt.subplots(2,2, figsize=(15,8))\n\nsns.distplot(sampling_distribution[\" Operating Gross Margin\"], ax=axes[0,0],label=\"sampling distribution of the mean\")\naxes[0,0].axvline(x=bankrupt_means[\" Operating Gross Margin\"],label=\"observation - pvalue %.2f\"%pvalues[\" Operating Gross Margin\"],c=\"r\")\naxes[0,0].legend(loc='upper left')\n\nsns.distplot(sampling_distribution[\" Interest-bearing debt interest rate\"], ax=axes[0,1])\naxes[0,1].axvline(x=bankrupt_means[\" Interest-bearing debt interest rate\"],label=\"pvalue %.2f\"%pvalues[\" Interest-bearing debt interest rate\"],c=\"r\")\naxes[0,1].legend()\n\nsns.distplot(sampling_distribution[\" Inventory/Current Liability\"], ax=axes[1,0])\naxes[1,0].axvline(x=bankrupt_means[\" Inventory/Current Liability\"],label=\"pvalue %.2f\"%pvalues[\" Inventory/Current Liability\"],c=\"r\")\naxes[1,0].legend()\n\nsns.distplot(sampling_distribution[\" No-credit Interval\"], ax=axes[1,1])\naxes[1,1].axvline(x=bankrupt_means[\" No-credit Interval\"],label=\"pvalue %.2f\"%pvalues[\" No-credit Interval\"],c=\"r\")\naxes[1,1].legend()\n\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:32:00.895959Z","iopub.execute_input":"2023-04-05T20:32:00.897034Z","iopub.status.idle":"2023-04-05T20:32:02.17949Z","shell.execute_reply.started":"2023-04-05T20:32:00.896993Z","shell.execute_reply":"2023-04-05T20:32:02.178269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I only exaimined the variables independently while there are probably many dependencies between them so I am not going to draw conclusions or perform feature selection based on these p-values.","metadata":{}},{"cell_type":"markdown","source":"# 4- Multicollinearity","metadata":{}},{"cell_type":"markdown","source":"I am going to find features with correlation coefficient greater than 0.9 and drop them.","metadata":{}},{"cell_type":"code","source":"#MULTICOLLINEARITY (CORRELATION BETWEEN PREDICTOR VARIABLES)\n\ncor_matrix = data.corr().abs()\ncor_matrix.style.background_gradient(sns.light_palette('red', as_cmap=True))","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:32:02.180851Z","iopub.execute_input":"2023-04-05T20:32:02.181215Z","iopub.status.idle":"2023-04-05T20:32:03.021216Z","shell.execute_reply.started":"2023-04-05T20:32:02.181169Z","shell.execute_reply":"2023-04-05T20:32:03.019561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dropping correlated data\n\nupper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(np.bool)) #upper triangle of the correlation matrix\n\ndropped_cols = set()\nfor feature in upper_tri.columns:\n    if any(upper_tri[feature] > 0.9): #more than 0.9 corr. coeficient -> dropped\n        dropped_cols.add(feature)\n\nprint(\"There are %d dropped columns\" %len(dropped_cols))\n\nX = X.drop(dropped_cols,axis=1)\nX.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:32:03.022771Z","iopub.execute_input":"2023-04-05T20:32:03.023137Z","iopub.status.idle":"2023-04-05T20:32:03.076963Z","shell.execute_reply.started":"2023-04-05T20:32:03.023103Z","shell.execute_reply":"2023-04-05T20:32:03.07527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**PCA** is a way to decorrelate and reduce the dimensionality of the data through the change of the basis. I am going to try if the method helps to decorrelate the data.","metadata":{}},{"cell_type":"code","source":"#PCA\n\nscaler = StandardScaler() \nX_for_pca = pd.DataFrame(data=scaler.fit_transform(X),index=X.index,columns=X.columns) #standardized dataset\n\nn_components = 10\n\npca = PCA(n_components=n_components)\nprincipal_components = pca.fit_transform(X_for_pca)\nX_pc = pd.DataFrame(data=principal_components, columns=['PC %d'%d for d in range(n_components)])\n\nprint(\"Explained variance by 10 components %.2f\" %sum(pca.explained_variance_ratio_))","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:32:03.079203Z","iopub.execute_input":"2023-04-05T20:32:03.079707Z","iopub.status.idle":"2023-04-05T20:32:03.156533Z","shell.execute_reply.started":"2023-04-05T20:32:03.079643Z","shell.execute_reply":"2023-04-05T20:32:03.155228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With 10 principal components the explained variance is still very low, so I do not find the PCA transformation useful for this data.","metadata":{}},{"cell_type":"markdown","source":"# 5- Data Imbalance","metadata":{}},{"cell_type":"markdown","source":"There is a huge imbalance between the data (only 3.2% companies from the dataset bankrupted). Before training a model I need to deal with this problem, otherwise the model would just predict every company to not bankrupt.\n\nI decided to try two ways:\n\n1- **Introducing weights** \\ Every datapoint from the minority class is considered \"more important\" than from the majority class, the weights for the two classes are inversely proportional to the number of datapoints in that class. Implemented within the SVM in next section.\n\n2- **SMOTE** \\ The Synthetic Minority Over-sampling TEchnique. \\ Creates new synthetic datapoints using the k-nearest neighbor algorithm. \\ With this method I am going to obtain the dataset where the value counts for both categories are the same.","metadata":{}},{"cell_type":"code","source":"#DATA IMBALANCE\n#SMOTE \n\nsm = SMOTE(random_state=42)\n\nX_sm, y_sm = sm.fit_resample(X, y)\n\nprint('New balance of 1 and 0 classes (%):')\ny_sm.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:32:03.158109Z","iopub.execute_input":"2023-04-05T20:32:03.158881Z","iopub.status.idle":"2023-04-05T20:32:03.275695Z","shell.execute_reply.started":"2023-04-05T20:32:03.158835Z","shell.execute_reply":"2023-04-05T20:32:03.273937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create list contain [accuracy,F1-measure, Recall, Precision]\nmod = []\naccuracy = []\n\nRecall = []\nPrecision = []\nF1_measure =[]","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:32:03.278647Z","iopub.execute_input":"2023-04-05T20:32:03.279891Z","iopub.status.idle":"2023-04-05T20:32:03.29177Z","shell.execute_reply.started":"2023-04-05T20:32:03.27982Z","shell.execute_reply":"2023-04-05T20:32:03.289773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm,test_size=0.2, stratify=y_sm) #stratify adresses the unbalance only in the train test splitting\n    ","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:32:03.300799Z","iopub.execute_input":"2023-04-05T20:32:03.301777Z","iopub.status.idle":"2023-04-05T20:32:03.325691Z","shell.execute_reply.started":"2023-04-05T20:32:03.301736Z","shell.execute_reply":"2023-04-05T20:32:03.324607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select  30 features\nX_train_10 = X_train.iloc[:,:10]\nX_test_10 = X_test.iloc[:,:10]\n\n# Select 50 features\nX_train_50 = X_train.iloc[:,:50]\nX_test_50 = X_test.iloc[:,:50]\n\n# Select 100 features\nX_train_70 = X_train.iloc[:,:70]\nX_test_70 = X_test.iloc[:,:70]","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:32:03.327289Z","iopub.execute_input":"2023-04-05T20:32:03.32791Z","iopub.status.idle":"2023-04-05T20:32:03.34155Z","shell.execute_reply.started":"2023-04-05T20:32:03.327874Z","shell.execute_reply":"2023-04-05T20:32:03.340076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1- SVM","metadata":{}},{"cell_type":"markdown","source":" am going to train a SVM model. First with SMOTE-dataset, then without SMOTE data and lastly with SMOTE-dataset but reduced to 10% of the data.\n\nThe function train_test_SVM(X,y) has multiple steps:\n\n1- Splitting the data               \n2- Assigning the weights                         \n3- Creating a Pipeline                                 \n4- Using GridSearchCV to find the optimal hyperparameters \\ Train the model                            \n5- Score                                         \n6- Confusion matrix\n\nThe SVM training takes quite long (around 4 minutes for me).\n\n* big amount of datapoints (perhaps too many for a SVM)\n* GridSearchCV using cross validation for different (C, gamma) combinations\n* training 'rbf' kernel is slower than linear kernel","metadata":{}},{"cell_type":"code","source":"#SVM\n\ndef train_test_SVM(X,X_test_):\n    \"\"\"Function finds the optimal hyperparameters of the SVM, plots the confusion matrix of test data, returns the model\"\"\"\n    #X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, stratify=y) #stratify adresses the unbalance only in the train test splitting\n    \n    sw_train = class_weight.compute_sample_weight(class_weight = 'balanced', y = y_train) #when balanced sw_train = [1.1.1...1]\n    \n    steps = [('scaler', StandardScaler()), ('SVM', SVC(cache_size=7000))]\n    pipeline = Pipeline(steps)\n    \n    #parameters' names must match the 'SVM' name in Pipeline followed by two underscores!\n    #standard SVM hyperparameters\n    param_grid = {\n    'SVM__C':[0.01,0.1,1,10],\n    'SVM__gamma':[0.1,0.01,0.001,0.0001],\n    'SVM__kernel':['rbf']\n    }\n    \n    f1 = make_scorer(f1_score , average='macro')\n    grid = GridSearchCV(pipeline,param_grid=param_grid, cv=5, scoring=f1, verbose=0) #verbose controls the training progression display!\n    grid.fit(X, y_train, SVM__sample_weight = sw_train)\n    \n    print(\"best parameters: \")\n    print(grid.best_params_)\n    \n    model = grid.best_estimator_\n    y_pred = model.predict(X_test_)\n    \n    print(\"f1 score is %.2f \"%f1_score(y_test, y_pred))\n    print(\"Precision: %.2f\" %precision_score(y_test, y_pred))\n    print(\"Recall: %.2f\" %recall_score(y_test, y_pred))\n    print(\"Precision: %.2f\" %precision_score(y_test, y_pred))\n    acc2 = accuracy_score(y_test, y_pred)\n    print(\"Accuracy score for SVM Model: {:.2f} %\".format(acc2*100))\n    plot_confusion_matrix(model,\n                         X_test_,\n                         y_test,\n                         values_format='d')\n    mod.append('SVM')\n    accuracy.append(acc2)\n    F1_measure.append(f1_score(y_test, y_pred))\n    Recall.append(recall_score(y_test, y_pred))\n    Precision.append(precision_score(y_test, y_pred))\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:32:03.343637Z","iopub.execute_input":"2023-04-05T20:32:03.344097Z","iopub.status.idle":"2023-04-05T20:32:03.357624Z","shell.execute_reply.started":"2023-04-05T20:32:03.344057Z","shell.execute_reply":"2023-04-05T20:32:03.356344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training and testing with SMOTE\n\nmodel = train_test_SVM(X_train_10,X_test_10)","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:32:03.359134Z","iopub.execute_input":"2023-04-05T20:32:03.359818Z","iopub.status.idle":"2023-04-05T20:36:42.305419Z","shell.execute_reply.started":"2023-04-05T20:32:03.359783Z","shell.execute_reply":"2023-04-05T20:36:42.304386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note: Without SMOTE the performance is a way worse. The model is not \"meaningless\" as it would be without the weights, however I suppose the weights are simply just \"not enough\" for such a big imbalance.","metadata":{}},{"cell_type":"code","source":"#Training and testing with SMOTE\n\nmodel = train_test_SVM(X_train_50,X_test_50)","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:36:42.306956Z","iopub.execute_input":"2023-04-05T20:36:42.307337Z","iopub.status.idle":"2023-04-05T20:41:42.382321Z","shell.execute_reply.started":"2023-04-05T20:36:42.307301Z","shell.execute_reply":"2023-04-05T20:41:42.381257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training and testing with SMOTE\n\nmodel = train_test_SVM(X_train_70,X_test_70)","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:41:42.383666Z","iopub.execute_input":"2023-04-05T20:41:42.383989Z","iopub.status.idle":"2023-04-05T20:48:03.223907Z","shell.execute_reply.started":"2023-04-05T20:41:42.383956Z","shell.execute_reply":"2023-04-05T20:48:03.223049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SVM does not use all the data to make a decision boundary, that is why the model works quite good with only 10% data. And the training is much faster.","metadata":{}},{"cell_type":"markdown","source":"# 2- Logistic Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, plot_confusion_matrix\nfrom sklearn.utils import class_weight\nfrom sklearn.utils.class_weight import compute_sample_weight","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:48:03.225305Z","iopub.execute_input":"2023-04-05T20:48:03.226088Z","iopub.status.idle":"2023-04-05T20:48:03.233471Z","shell.execute_reply.started":"2023-04-05T20:48:03.22604Z","shell.execute_reply":"2023-04-05T20:48:03.232071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_test_LogisticRegression(X_train,X_test_):\n    \"\"\"Function finds the optimal hyperparameters of the logistic regression, plots the confusion matrix of test data, returns the model\"\"\"\n    #X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, stratify=y) #stratify addresses the unbalance only in the train test splitting\n    \n    sw_train = compute_sample_weight(class_weight = 'balanced', y = y_train) #when balanced sw_train = [1.1.1...1]\n    \n    steps = [('scaler', StandardScaler()), ('LogisticRegression', LogisticRegression(solver='lbfgs', max_iter=10000))]\n    pipeline = Pipeline(steps)\n    \n    #parameters' names must match the 'LogisticRegression' name in Pipeline followed by two underscores!\n    #standard logistic regression hyperparameters\n    param_grid = {\n        'LogisticRegression__C': [0.01, 0.1, 1, 10],\n        'LogisticRegression__penalty': ['l1', 'l2'],\n        'LogisticRegression__class_weight': ['balanced', None]\n    }\n    \n    f1 = make_scorer(f1_score , average='macro')\n    grid = GridSearchCV(pipeline, param_grid=param_grid, cv=5, scoring=f1, verbose=0) #verbose controls the training progression display!\n    grid.fit(X_train, y_train, LogisticRegression__sample_weight=sw_train)\n    \n    print(\"best parameters: \")\n    print(grid.best_params_)\n    \n    model = grid.best_estimator_\n    y_pred = model.predict(X_test_)\n    \n    print(\"f1 score is %.2f \"%f1_score(y_test, y_pred))\n    print(\"Precision: %.2f\" %precision_score(y_test, y_pred))\n    print(\"Recall: %.2f\" %recall_score(y_test, y_pred))\n    acc = accuracy_score(y_test, y_pred)\n    print(\"Accuracy score for Logistic Regression Model: {:.2f} %\".format(acc*100))\n    plot_confusion_matrix(model,\n                         X_test_,\n                         y_test,\n                         values_format='d')\n    mod.append('LR')\n    accuracy.append(acc)\n    F1_measure.append(f1_score(y_test, y_pred))\n    Recall.append(recall_score(y_test, y_pred))\n    Precision.append(precision_score(y_test, y_pred))\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:48:03.235071Z","iopub.execute_input":"2023-04-05T20:48:03.235407Z","iopub.status.idle":"2023-04-05T20:48:03.248874Z","shell.execute_reply.started":"2023-04-05T20:48:03.235376Z","shell.execute_reply":"2023-04-05T20:48:03.247931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training and testing with SMOTE\n\nmodel = train_test_LogisticRegression(X_train_10,X_test_10)","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:48:03.250258Z","iopub.execute_input":"2023-04-05T20:48:03.250538Z","iopub.status.idle":"2023-04-05T20:48:06.695767Z","shell.execute_reply.started":"2023-04-05T20:48:03.250511Z","shell.execute_reply":"2023-04-05T20:48:06.694553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = train_test_LogisticRegression(X_train_50,X_test_50)","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:48:06.703442Z","iopub.execute_input":"2023-04-05T20:48:06.704482Z","iopub.status.idle":"2023-04-05T20:48:17.235961Z","shell.execute_reply.started":"2023-04-05T20:48:06.704442Z","shell.execute_reply":"2023-04-05T20:48:17.235091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = train_test_LogisticRegression(X_train_70,X_test_70)","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:48:17.237344Z","iopub.execute_input":"2023-04-05T20:48:17.237688Z","iopub.status.idle":"2023-04-05T20:48:32.007286Z","shell.execute_reply.started":"2023-04-05T20:48:17.237655Z","shell.execute_reply":"2023-04-05T20:48:32.006409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. RandomForest ","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer, f1_score, precision_score, recall_score, accuracy_score\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\ndef train_test_RandomForest(X, X_test_):\n    \"\"\"Function finds the optimal hyperparameters of the Random Forest Classifier, plots the confusion matrix of test data, and returns the model\"\"\"\n    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n    \n    sw_train = compute_sample_weight(class_weight='balanced', y=y_train)\n    \n    steps = [('scaler', StandardScaler()), ('RandomForest', RandomForestClassifier())]\n    pipeline = Pipeline(steps)\n    \n    # Random Forest Classifier hyperparameters\n    param_grid = {\n        'RandomForest__n_estimators': [10, 20, 30],\n        'RandomForest__max_depth': [None, 10, 20],\n        'RandomForest__min_samples_split': [2, 5],\n        'RandomForest__class_weight': ['balanced', None]\n    }\n    \n    f1 = make_scorer(f1_score, average='macro')\n    grid = GridSearchCV(pipeline, param_grid=param_grid, cv=5, scoring=f1, verbose=0)\n    grid.fit(X, y_train, RandomForest__sample_weight=sw_train)\n    \n    print(\"best parameters: \")\n    print(grid.best_params_)\n    \n    model = grid.best_estimator_\n    y_pred = model.predict(X_test_)\n    \n    print(\"f1 score is %.2f\" % f1_score(y_test, y_pred))\n    print(\"Precision: %.2f\" % precision_score(y_test, y_pred))\n    print(\"Recall: %.2f\" % recall_score(y_test, y_pred))\n    acc = accuracy_score(y_test, y_pred)\n    print(\"Accuracy score for Random Forest Model: {:.2f} %\".format(acc*100))\n    plot_confusion_matrix(model, X_test_, y_test, values_format='d')\n    mod.append('RF')\n    accuracy.append(acc)\n    F1_measure.append(f1_score(y_test, y_pred))\n    Recall.append(recall_score(y_test, y_pred))\n    Precision.append(precision_score(y_test, y_pred))\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:48:32.009081Z","iopub.execute_input":"2023-04-05T20:48:32.00955Z","iopub.status.idle":"2023-04-05T20:48:32.094359Z","shell.execute_reply.started":"2023-04-05T20:48:32.009504Z","shell.execute_reply":"2023-04-05T20:48:32.093417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training and testing with SMOTE\n\nmodel = train_test_RandomForest(X_train_10,X_test_10)","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:48:32.095514Z","iopub.execute_input":"2023-04-05T20:48:32.095814Z","iopub.status.idle":"2023-04-05T20:49:40.067661Z","shell.execute_reply.started":"2023-04-05T20:48:32.095785Z","shell.execute_reply":"2023-04-05T20:49:40.066382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training and testing with SMOTE\n\nmodel = train_test_RandomForest(X_train_50,X_test_50)","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:49:40.06922Z","iopub.execute_input":"2023-04-05T20:49:40.06965Z","iopub.status.idle":"2023-04-05T20:51:59.16502Z","shell.execute_reply.started":"2023-04-05T20:49:40.069604Z","shell.execute_reply":"2023-04-05T20:51:59.164202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training and testing with SMOTE\n\nmodel = train_test_RandomForest(X_train_70,X_test_70)","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:51:59.166203Z","iopub.execute_input":"2023-04-05T20:51:59.166726Z","iopub.status.idle":"2023-04-05T20:54:42.006601Z","shell.execute_reply.started":"2023-04-05T20:51:59.166693Z","shell.execute_reply":"2023-04-05T20:54:42.005381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4- NaiveBayes","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, plot_confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:54:42.008109Z","iopub.execute_input":"2023-04-05T20:54:42.009033Z","iopub.status.idle":"2023-04-05T20:54:42.021327Z","shell.execute_reply.started":"2023-04-05T20:54:42.008997Z","shell.execute_reply":"2023-04-05T20:54:42.020196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, plot_confusion_matrix\nfrom sklearn.utils.class_weight import compute_sample_weight\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\n\ndef train_test_NaiveBayes(X, X_test_):\n    \"\"\"Function finds the optimal hyperparameters of the Naive Bayes classifier, plots the confusion matrix of test data, and returns the model\"\"\"\n    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y) #stratify addresses the unbalance only in the train test splitting\n\n    sw_train = compute_sample_weight(class_weight='balanced', y=y_train) #when balanced sw_train = [1.1.1...1]\n\n    steps = [('scaler', StandardScaler()), ('NaiveBayes', GaussianNB())]\n    pipeline = Pipeline(steps)\n    grid_param = {'NaiveBayes__var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]}\n\n    # No hyperparameters to optimize for Naive Bayes\n\n    f1 = make_scorer(f1_score , average='macro')\n    grid = GridSearchCV(pipeline, param_grid=grid_param, cv=5, scoring=f1, verbose=0) #verbose controls the training progression display!\n    grid.fit(X, y_train, NaiveBayes__sample_weight=sw_train)\n\n    print(\"best parameters: \")\n    print(grid.best_params_)\n\n    model = grid.best_estimator_\n    y_pred = model.predict(X_test_)\n\n    print(\"f1 score is %.2f \"%f1_score(y_test, y_pred))\n    print(\"Precision: %.2f\" %precision_score(y_test, y_pred))\n    print(\"Recall: %.2f\" %recall_score(y_test, y_pred))\n    acc = accuracy_score(y_test, y_pred)\n    print(\"Accuracy score for Naive Bayes Model: {:.2f} %\".format(acc*100))\n    plot_confusion_matrix(model, X_test_, y_test, values_format='d')\n    mod.append('NB')\n    accuracy.append(acc)\n    F1_measure.append(f1_score(y_test, y_pred))\n    Recall.append(recall_score(y_test, y_pred))\n    Precision.append(precision_score(y_test, y_pred))\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:54:42.023048Z","iopub.execute_input":"2023-04-05T20:54:42.024122Z","iopub.status.idle":"2023-04-05T20:54:42.039586Z","shell.execute_reply.started":"2023-04-05T20:54:42.024084Z","shell.execute_reply":"2023-04-05T20:54:42.038543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training and testing with SMOTE\n\nmodel = train_test_NaiveBayes(X_train_10,X_test_10)","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:54:42.041584Z","iopub.execute_input":"2023-04-05T20:54:42.042Z","iopub.status.idle":"2023-04-05T20:54:42.635578Z","shell.execute_reply.started":"2023-04-05T20:54:42.041956Z","shell.execute_reply":"2023-04-05T20:54:42.634374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training and testing with SMOTE\n\nmodel = train_test_NaiveBayes(X_train_50,X_test_50)","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:54:42.637092Z","iopub.execute_input":"2023-04-05T20:54:42.637957Z","iopub.status.idle":"2023-04-05T20:54:43.443418Z","shell.execute_reply.started":"2023-04-05T20:54:42.637923Z","shell.execute_reply":"2023-04-05T20:54:43.442297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training and testing with SMOTE\n\nmodel = train_test_NaiveBayes(X_train_70,X_test_70)","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:54:43.444508Z","iopub.execute_input":"2023-04-05T20:54:43.444799Z","iopub.status.idle":"2023-04-05T20:54:44.4102Z","shell.execute_reply.started":"2023-04-05T20:54:43.444771Z","shell.execute_reply":"2023-04-05T20:54:44.409065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5- DecisionTree","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:54:44.411446Z","iopub.execute_input":"2023-04-05T20:54:44.411752Z","iopub.status.idle":"2023-04-05T20:54:44.416834Z","shell.execute_reply.started":"2023-04-05T20:54:44.411721Z","shell.execute_reply":"2023-04-05T20:54:44.415636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndef train_test_DecisionTree(X, X_test_):\n    \"\"\"Function finds the optimal hyperparameters of the Decision Tree classifier, plots the confusion matrix of test data, and returns the model\"\"\"\n    sw_train = compute_sample_weight(class_weight='balanced', y=y_train) \n\n    steps = [('scaler', StandardScaler()), ('DecisionTree', DecisionTreeClassifier(random_state=42))]\n    pipeline = Pipeline(steps)\n\n    grid_param = {'DecisionTree__max_depth': [5, 10, 15, 20, 25],\n                  'DecisionTree__min_samples_split': [2, 5, 10, 15, 20],\n                  'DecisionTree__min_samples_leaf': [1, 2, 5, 10, 15]}\n\n    f1 = make_scorer(f1_score, average='macro')\n    grid = GridSearchCV(pipeline, param_grid=grid_param, cv=5, scoring=f1, verbose=0)\n    grid.fit(X, y_train, DecisionTree__sample_weight=sw_train)\n\n    print(\"best parameters: \")\n    print(grid.best_params_)\n\n    model = grid.best_estimator_\n    y_pred = model.predict(X_test_)\n\n    print(\"f1 score is %.2f \"%f1_score(y_test, y_pred))\n    print(\"Precision: %.2f\" %precision_score(y_test, y_pred))\n    print(\"Recall: %.2f\" %recall_score(y_test, y_pred))\n    acc = accuracy_score(y_test, y_pred)\n    print(\"Accuracy score for Decision Tree Model: {:.2f} %\".format(acc*100))\n    plot_confusion_matrix(model, X_test_, y_test, values_format='d')\n    mod.append('DT')\n    accuracy.append(acc)\n    F1_measure.append(f1_score(y_test, y_pred))\n    Recall.append(recall_score(y_test, y_pred))\n    Precision.append(precision_score(y_test, y_pred))\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:54:44.418409Z","iopub.execute_input":"2023-04-05T20:54:44.418844Z","iopub.status.idle":"2023-04-05T20:54:44.432097Z","shell.execute_reply.started":"2023-04-05T20:54:44.418773Z","shell.execute_reply":"2023-04-05T20:54:44.430947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training and testing with SMOTE\n\nmodel = train_test_DecisionTree(X_train_10,X_test_10)","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:54:44.433921Z","iopub.execute_input":"2023-04-05T20:54:44.434578Z","iopub.status.idle":"2023-04-05T20:55:33.387366Z","shell.execute_reply.started":"2023-04-05T20:54:44.434533Z","shell.execute_reply":"2023-04-05T20:55:33.386331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = train_test_DecisionTree(X_train_50,X_test_50)","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:55:33.388467Z","iopub.execute_input":"2023-04-05T20:55:33.388762Z","iopub.status.idle":"2023-04-05T20:59:19.397202Z","shell.execute_reply.started":"2023-04-05T20:55:33.388733Z","shell.execute_reply":"2023-04-05T20:59:19.396241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = train_test_DecisionTree(X_train_70,X_test_70)","metadata":{"execution":{"iopub.status.busy":"2023-04-05T20:59:19.398571Z","iopub.execute_input":"2023-04-05T20:59:19.398986Z","iopub.status.idle":"2023-04-05T21:04:52.358428Z","shell.execute_reply.started":"2023-04-05T20:59:19.398949Z","shell.execute_reply":"2023-04-05T21:04:52.356633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6.AdaBoost","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\ndef train_test_AdaBoost(X,X_test_):\n    \"\"\"Function finds the optimal hyperparameters of AdaBoost, plots the confusion matrix of test data, returns the model\"\"\"\n    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y) # stratify adresses the unbalance only in the train test splitting\n    \n    sw_train = class_weight.compute_sample_weight(class_weight='balanced', y=y_train) # when balanced sw_train = [1.1.1...1]\n    \n    pipeline = Pipeline([('scaler', StandardScaler()), ('AdaBoost', AdaBoostClassifier())])\n    \n    # AdaBoost hyperparameters\n    param_grid = {\n        'AdaBoost__n_estimators': [50, 100, 200],\n        'AdaBoost__learning_rate': [0.01, 0.1, 1]\n    }\n    \n    f1 = make_scorer(f1_score, average='macro')\n    grid = GridSearchCV(pipeline, param_grid=param_grid, cv=5, scoring=f1, verbose=0) # verbose controls the training progression display!\n    grid.fit(X, y_train, AdaBoost__sample_weight=sw_train)\n    \n    print(\"best parameters: \")\n    print(grid.best_params_)\n    \n    model = grid.best_estimator_\n    y_pred = model.predict(X_test_)\n    \n    print(\"f1 score is %.2f\" % f1_score(y_test, y_pred))\n    print(\"Precision: %.2f\" % precision_score(y_test, y_pred))\n    print(\"Recall: %.2f\" % recall_score(y_test, y_pred))\n    print(\"Accuracy: %.2f\" % accuracy_score(y_test, y_pred))\n    \n    plot_confusion_matrix(model, X_test_, y_test, values_format='d')\n    \n    mod.append('AdaBoost')\n    accuracy.append(accuracy_score(y_test, y_pred))\n    F1_measure.append(f1_score(y_test, y_pred))\n    Recall.append(recall_score(y_test, y_pred))\n    Precision.append(precision_score(y_test, y_pred))\n    \n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-04-05T21:04:52.361568Z","iopub.execute_input":"2023-04-05T21:04:52.36206Z","iopub.status.idle":"2023-04-05T21:04:52.374253Z","shell.execute_reply.started":"2023-04-05T21:04:52.36201Z","shell.execute_reply":"2023-04-05T21:04:52.37284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = train_test_AdaBoost(X_train_10,X_test_10)","metadata":{"execution":{"iopub.status.busy":"2023-04-05T21:04:52.375401Z","iopub.execute_input":"2023-04-05T21:04:52.376627Z","iopub.status.idle":"2023-04-05T21:06:09.100461Z","shell.execute_reply.started":"2023-04-05T21:04:52.376582Z","shell.execute_reply":"2023-04-05T21:06:09.098287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = train_test_AdaBoost(X_train_50,X_test_50)","metadata":{"execution":{"iopub.status.busy":"2023-04-05T21:06:09.102245Z","iopub.execute_input":"2023-04-05T21:06:09.102847Z","iopub.status.idle":"2023-04-05T21:10:29.963322Z","shell.execute_reply.started":"2023-04-05T21:06:09.102806Z","shell.execute_reply":"2023-04-05T21:10:29.961748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = train_test_AdaBoost(X_train_70,X_test_70)","metadata":{"execution":{"iopub.status.busy":"2023-04-05T21:10:29.964698Z","iopub.execute_input":"2023-04-05T21:10:29.965032Z","iopub.status.idle":"2023-04-05T21:16:34.216519Z","shell.execute_reply.started":"2023-04-05T21:10:29.965Z","shell.execute_reply":"2023-04-05T21:16:34.215648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. GradientBoostingClassifier\n","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\ndef train_test_GBT(X, X_test_):\n    \"\"\"Function finds the optimal hyperparameters of the Gradient Boosted Trees classifier, plots the confusion matrix of test data, and returns the model\"\"\"\n    sw_train = compute_sample_weight(class_weight='balanced', y=y_train) \n\n    steps = [('scaler', StandardScaler()), ('GBT', GradientBoostingClassifier(random_state=42))]\n    pipeline = Pipeline(steps)\n\n    grid_param = {'GBT__n_estimators': [50, 100, 200],\n                  'GBT__learning_rate': [0.1, 0.05, 0.01],\n                  'GBT__max_depth': [3, 5, 7]}\n\n    f1 = make_scorer(f1_score, average='macro')\n    grid = GridSearchCV(pipeline, param_grid=grid_param, cv=5, scoring=f1, verbose=0)\n    grid.fit(X, y_train, GBT__sample_weight=sw_train)\n\n    print(\"best parameters: \")\n    print(grid.best_params_)\n\n    model = grid.best_estimator_\n    y_pred = model.predict(X_test_)\n\n    print(\"f1 score is %.2f \"%f1_score(y_test, y_pred))\n    print(\"Precision: %.2f\" %precision_score(y_test, y_pred))\n    print(\"Recall: %.2f\" %recall_score(y_test, y_pred))\n    acc = accuracy_score(y_test, y_pred)\n    print(\"Accuracy score for Gradient Boosted Trees Model: {:.2f} %\".format(acc*100))\n    plot_confusion_matrix(model, X_test_, y_test, values_format='d')\n    mod.append('GBT')\n    accuracy.append(acc)\n    F1_measure.append(f1_score(y_test, y_pred))\n    Recall.append(recall_score(y_test, y_pred))\n    Precision.append(precision_score(y_test, y_pred))\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-04-05T21:16:34.217663Z","iopub.execute_input":"2023-04-05T21:16:34.21801Z","iopub.status.idle":"2023-04-05T21:16:34.229574Z","shell.execute_reply.started":"2023-04-05T21:16:34.217976Z","shell.execute_reply":"2023-04-05T21:16:34.22842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = train_test_GBT(X_train_10,X_test_10)","metadata":{"execution":{"iopub.status.busy":"2023-04-05T21:16:34.230736Z","iopub.execute_input":"2023-04-05T21:16:34.231491Z","iopub.status.idle":"2023-04-05T21:26:41.834027Z","shell.execute_reply.started":"2023-04-05T21:16:34.231457Z","shell.execute_reply":"2023-04-05T21:26:41.832902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = train_test_GBT(X_train_50,X_test_50)","metadata":{"execution":{"iopub.status.busy":"2023-04-05T21:26:41.837051Z","iopub.execute_input":"2023-04-05T21:26:41.837785Z","iopub.status.idle":"2023-04-05T22:10:16.528795Z","shell.execute_reply.started":"2023-04-05T21:26:41.83775Z","shell.execute_reply":"2023-04-05T22:10:16.527507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = train_test_GBT(X_train_70,X_test_70)","metadata":{"execution":{"iopub.status.busy":"2023-04-05T22:10:16.530311Z","iopub.execute_input":"2023-04-05T22:10:16.530646Z","iopub.status.idle":"2023-04-05T23:11:43.440106Z","shell.execute_reply.started":"2023-04-05T22:10:16.530615Z","shell.execute_reply":"2023-04-05T23:11:43.438928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame({'Classifier': mod, 'Accuracy': accuracy, 'F1-Measure': F1_measure,'Recall': Recall,'Precision':Precision})\n","metadata":{"execution":{"iopub.status.busy":"2023-04-05T23:30:35.343093Z","iopub.execute_input":"2023-04-05T23:30:35.343517Z","iopub.status.idle":"2023-04-05T23:30:35.350083Z","shell.execute_reply.started":"2023-04-05T23:30:35.343481Z","shell.execute_reply":"2023-04-05T23:30:35.348915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2023-04-05T23:30:35.746687Z","iopub.execute_input":"2023-04-05T23:30:35.747062Z","iopub.status.idle":"2023-04-05T23:30:35.76233Z","shell.execute_reply.started":"2023-04-05T23:30:35.747029Z","shell.execute_reply":"2023-04-05T23:30:35.760991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2=df","metadata":{"execution":{"iopub.status.busy":"2023-04-05T23:30:36.957601Z","iopub.execute_input":"2023-04-05T23:30:36.957986Z","iopub.status.idle":"2023-04-05T23:30:36.962927Z","shell.execute_reply.started":"2023-04-05T23:30:36.957954Z","shell.execute_reply":"2023-04-05T23:30:36.961828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dictionary to map the old Classifier names to the new names\n\n# Add a new column with the SMOTE information\ndf2['Feature'] = ['_50' if i%3==1 else '_70' if i%3==2 else '_10' for i in range(len(df2))]\n\n# Merge the Classifier and SMOTE columns to get the desired output\ndf2['Classifier'] = df2['Classifier'] +  df2['Feature']\n# Drop the SMOTE column\ndf2 = df2.drop('Feature', axis=1)\n# Print the updated dataframe\ndf2\n","metadata":{"execution":{"iopub.status.busy":"2023-04-05T23:30:38.429704Z","iopub.execute_input":"2023-04-05T23:30:38.430094Z","iopub.status.idle":"2023-04-05T23:30:38.449528Z","shell.execute_reply.started":"2023-04-05T23:30:38.430061Z","shell.execute_reply":"2023-04-05T23:30:38.448664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Split the dataframe into groups of 3 rows\ngroups = df2.groupby(df2.index // 3)\n\n# Iterate over the groups and create individual charts\nfor i, group in groups:\n    ax = group.plot(kind='bar', figsize=(10,6))\n    ax.set_title(f'Classifier Comparison')\n    ax.set_xlabel('Classifier')\n    ax.set_ylabel('Score')\n    ax.set_xticklabels(group[\"Classifier\"],rotation=0)\n    plt.tight_layout()  # add this line to adjust spacing\n    plt.tight_layout()  # add this line to adjust spacing\n    plt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-05T23:38:55.932987Z","iopub.execute_input":"2023-04-05T23:38:55.933403Z","iopub.status.idle":"2023-04-05T23:38:58.002819Z","shell.execute_reply.started":"2023-04-05T23:38:55.933366Z","shell.execute_reply":"2023-04-05T23:38:58.001717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\n# Set the index of the dataframe to the Classifier column\ndf2.set_index('Classifier', inplace=True)\n\n# Plot the bar chart\nax = df2.plot(kind='bar', figsize=(10,6))\n\n# Set the title, x-label, and y-label of the plot\nax.set_title('Classifier Comparison')\nax.set_xlabel('Classifier')\nax.set_ylabel('Score')\n\n# Show the plot\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-04-05T23:23:18.914224Z","iopub.execute_input":"2023-04-05T23:23:18.914666Z","iopub.status.idle":"2023-04-05T23:23:19.402802Z","shell.execute_reply.started":"2023-04-05T23:23:18.914614Z","shell.execute_reply":"2023-04-05T23:23:19.401552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}